# Anonymized Human Judge Scores Tally

This table summarizes the review scores for each project, by each human judge (anonymized as S, G, T), for all criteria and total scores. All scores are normalized to a 1–5 scale.

| Project                | Judge | Innovation & Creativity (25%) | Impact & Relevance (25%) | Technical Excellence (20%) | Usability & Accessibility (15%) | Ethics & Fairness (15%) | **Total Score** |
|------------------------|-------|------------------------|--------------------|---------------------|--------------------------|-------------------|:--------------:|
| **Behind the Bill**    | S     | 4.0                    | 4.0                | 3.0                 | 4.0                      | 3.0               | 3.65           |
|                        | G     | 4.0                    | 4.5                | 4.5                 | 3.5                      | 4.0               | 4.15           |
|                        | T     | 4.0                    | 4.0                | 3.0                 | 4.0                      | 3.0               | 3.65           |
| **Billinguo**          | S     | 3.0                    | 4.0                | 4.0                 | 5.0                      | 3.0               | 3.75           |
|                        | G     | 3.5                    | 3.5                | 2.5                 | 3.5                      | 2.5               | 3.15           |
|                        | T     | 4.0                    | 4.0                | 4.0                 | 5.0                      | 3.0               | 4.00           |
| **DebateSim**          | S     | 5.0                    | 5.0                | 5.0                 | 4.0                      | 5.0               | 4.85           |
|                        | G     | 3.5                    | 4.0                | 4.0                 | 3.5                      | 3.5               | 3.73           |
|                        | T     | 5.0                    | 4.0                | 5.0                 | 4.0                      | 4.0               | 4.45           |
| **LegisCompareAI**     | S     | 3.0                    | 5.0                | 3.0                 | 5.0                      | 3.0               | 3.80           |
|                        | G     | 4.0                    | 4.0                | 3.5                 | 4.0                      | 4.0               | 3.90           |
|                        | T     | 3.0                    | 3.0                | 4.0                 | 4.0                      | 3.0               | 3.35           |
| **Predictive Bill Tracker** | S | 3.0                    | 4.0                | 5.0                 | 5.0                      | 5.0               | 4.25           |
|                        | G     | 4.5                    | 4.5                | 5.0                 | 4.0                      | 4.0               | 4.45           |
|                        | T     | 5.0                    | 5.0                | 5.0                 | 5.0                      | 5.0               | 5.00           |

---

## Award Score Ranges

- **First Level Award:** 4.5 – 5.0
- **Second Level Award:** 4.0 – 4.49
- **Third Level Award:** 3.5 – 3.99

---

## Final Recommended Awards (Based on Human Judge Averages)

| Project                 | Human Judges Average Score | Award Level         |
|-------------------------|:-------------------------:|:-------------------:|
| Predictive Bill Tracker (Oliver Fan) | 4.57                      | First Level Award   |
| DebateSim (Alex Liao, et al.)              | 4.34                      | Second Level Award  |
| Behind the Bill (Lina Iyer)        | 3.82                      | Third Level Award   |
| LegisCompareAI (Mark Garcia)         | 3.68                      | Third Level Award   |
| Billinguo (Ian Lee)              | 3.63                      | Third Level Award   |


